{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting primordial non-gaussianity without cosmic variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Non-gaussianity in the initial conditions of the universe is one of the most powerful mechanisms to discriminate among the competing theories of the early universe. Measurements using bispectrum of cosmic microwave background anisotropies are limited by the cosmic variance, i.e. available number of modes. Recent work has emphasized the possibility to probe non-gaussianity of local type using the scale dependence of large scale bias from highly biased tracers of large scale structure. However, this power spectrum method is also limited by cosmic variance, finite number of structures on the largest scales, and bu the partial degeneracy with other cosmological parameters that can mimic the same effect. Here we propose an alternative method that solves both of these problems. It is based on the idea that on large scales halos are biased, but not stochastic, tracers of dark matter: by correlating a highly biased tracer of large scale structure against an unbiased tracer one eliminates the cosmic variance error, which van lead to a high signal to noise even from teh structures comparable to the size of the survey. The error improvement on non-gaussianity parameter $f_{nl}$ relative to the power spectrum method scales as $(P\\bar{n}/2)^{1/2}$, where $P$ and $\\bar{n}$ is the power spectrum and the number desnity of the biased tracer, respectively. For luminous red galaxies at z ~ 0.5 we get an error improvement of about a factor of three over the current constrainsts, assuming unbiased tracers are also available with a high number desnity. For an ideal survey out to z ~ 2 the corresponding error reduction can be as large as a factor of seven, which should guarantee a detection of non-gaussianity from an all sky survey of this type. The improvements could be even larger if two high denisty tracers with a significantly different sensitivity to non-gaussianity can be identified and measured over a large volume.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the abstract taken from Uros Seljak's paper. Essentially, if we imagine that we have some random variable with some distribution and we have two things that depend on the same outcome of that random variable, then we can take the ratio of the two outcomes to eliminate the randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a block diagonal matrix, in which $f_{nl}$ is its own block, then the inverse is very easy to calculate. It is simply $1/f_{nl}$. However, the brute force method of calculating the fisher matrix based on the two biases and the non-gaussianity will not produce this nice block diagonal matrix. Instead, Uros uses a change of bases to get the matrix in this form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used to confirm that the brute force method produces results that will agree with Uros's way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_{ij} = \\frac{1}{2} tr[C_{,i} C^{-1} C_{,j} C^{-1}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where C is the covariance matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C = \\begin{pmatrix}\n",
    "    (\\alpha^2 P_2 + \\bar{n}_1^{-1})/V & r\\alpha P_2/V \\\\\n",
    "    r\\alpha P_2/V & (P_2 + \\bar{n}_2^{-1})/V\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\alpha$ is the relative bias $\\alpha = b_1/b_2$, V is the volume element (which drops out from the final expressions), $P_2$ is the power spectrum of the second tracer, and r is the cross-correlation coefficient between the two fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uros uses parameters, $P_2$ and $\\alpha$, and their dependence on $f_{nl}$ to compute the errors on $f_{nl}$. I will instead have to calculate the full Fisher matrix and invert it to find the error on $f_{nl}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple model for the power spectrum:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_{ij}(k) = \\left( b_i + \\frac{b_i f_{nl}}{k^2}\\right) \\left( b_j + \\frac{b_j f_{nl}}{k^2} \\right) P_{dm}(k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the future:\n",
    "\n",
    "$$P_{ij}(k) = b_i b_j \\left( 1 + \\frac{f_{nl}}{k^\\nu} \\right)^2 P_{dm}(k)$$\n",
    "\n",
    "$$P_{ij}(k) = \\left( b_i + \\tilde{b}k^2 + \\frac{b_i f_{nl}}{k^\\nu} \\right) \\left( b_j + \\tilde{b}k^2 + \\frac{b_j f_{nl}}{k^\\nu} \\right) P_{dm}(k)$$\n",
    "\n",
    "In these cases, the derivatives will still be 2x2 matrices, but then the whole Fisher matrix will be 4x4 and 5x5 respectively. In the first case, we add the parameter $\\nu$ and in the second case we add $\\nu$ and $\\tilde{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in mind, we can rewrite the covariance matrix as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C = \\begin{pmatrix}\n",
    "    (P_{11}(k) + \\bar{n}_1^{-1})/V & r P_{12}(k)/V \\\\\n",
    "    r P_{12}(k)/V & (P_{22}(k) + \\bar{n}_2^{-1})/V\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make life easier, we can drop the volume scalar because when we invert the matrix and multiply it by some partial derivative of the covariance matrix, the volume will cancel. Here are the derivatives with the volume factor removed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C_{,b_1} = \\frac{\\partial C}{\\partial b_1} = \\left( 1 + \\frac{f_{nl}}{k^2} \\right)^2 P_{dm}(k)\\begin{pmatrix}\n",
    "            2b_1 & rb_2 \\\\\n",
    "            rb_2 & 0\n",
    "        \\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "C_{,b_2} = \\frac{\\partial C}{\\partial b_2} = \\left( 1 + \\frac{f_{nl}}{k^2} \\right)^2 P_{dm}(k)\\begin{pmatrix}\n",
    "            0 & rb_1 \\\\\n",
    "            rb_1 & 2b_2\n",
    "        \\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "C_{,f_{nl}} = \\frac{\\partial C}{\\partial f_{nl}} = \\left( \\frac{2}{k^2} + \\frac{2f_{nl}}{k^4} \\right)P_{dm}(k)\\begin{pmatrix}\n",
    "            b_1^2 & rb_1b_2 \\\\\n",
    "            rb_1b_2 & b_2^2\n",
    "        \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the analytical expression for the inverse, however, we could have easily made the computer compute the inverse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C^{-1} = \\frac{1}{(P_{11}(k) + \\bar n_1^{-1})(P_{22}(k)+\\bar n_2^{-1}) - r^2 P_{12}^2(k)}\\begin{pmatrix}\n",
    "            (P_{22}(k)+\\bar n_2^{-1}) & -rP_{12}(k) \\\\\n",
    "            -rP_{12}(k) & (P_{11}(k)+\\bar n_1^{-1})\n",
    "        \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Uros's paper, he assumes a couple of conditions to help simplify the calculations. First he defines $X_i = (\\bar{n}_i P_2)^{-1}$. If $X_i$ is greater than one, then we are in the Poisson noise dominated limit and Poisson noise dominates over the sampling variance. In the opposite limit $X_i \\ll 1$ sampling variance dominates the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import multi_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for the Fisher matrix:\n",
    "\n",
    "It will take in an array of derivatives and the covariance matrix. Then it will compute the inverse of the covariance matrix, perform the necessary matrix multiplication and finally take the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher(derivatives, covariance):\n",
    "    inv_cov = covariance.I\n",
    "    \n",
    "    size = len(derivatives)\n",
    "    fmatrix = np.zeros(shape=(size,size))\n",
    "    \n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            fmatrix[i][j] = (1/2)*np.trace(multi_dot([derivatives[i], inv_cov, derivatives[j], inv_cov]))\n",
    "    return fmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will take in the necessary arguments, and compute all the covariance matrices for the different values of k. It will output a dictionary with each key being a value of k and their respective entries are the covariance matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(b1, b2, r, fnl, k_array, Pdm, n1, n2):\n",
    "    #different cmatrix for each k?\n",
    "    cmatrix_dict = {}\n",
    "    \n",
    "    for k in k_array:\n",
    "        P11 = b1**2(1 + fnl/(k**2))**2*Pdm[k] #assumption that the Pdm is a dictionary\n",
    "        P12 = b1*b2(1 + fnl/(k**2))**2*Pdm[k]\n",
    "        P22 = b2**2(1 + fnl/(k**2))**2*Pdm[k]\n",
    "        cmatrix_dict[k] = [[P11 + 1/n1, r*P12],[r*P12, P22 + 1/n2]]\n",
    "    return cmatrix_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method calculates the derivatives in the simple example where we differentiate with respect to $b_1$, $b_2$, and $f_{nl}$. It outputs a list of dictionaries where the keys of the dictionaries are the different values of k and the entries are the derivatives matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivatives_1(b1, b2, r, fnl, k_array, Pdm):\n",
    "    #arrays of the partial derivative matrices\n",
    "    Cb1 = {}\n",
    "    Cb2 = {}\n",
    "    Cfnl = {}\n",
    "    \n",
    "    for k in k_array:\n",
    "        constant_b1 = (1 + fnl/(k**2))**2*Pdm[k]\n",
    "        Cb1[k] = [[constant_b1*(2*b1), constant_b1*(r*b2)], [constant_b1*(r*b2), 0]]\n",
    "        constant_b2 = (1 + fnl/(k**2))**2*Pdm[k]\n",
    "        Cb2[k] = [[0, constant_b2*(r*b1)], [constant_b2*(r*b1), constant_b2*(2*b2)]]\n",
    "        constant_fnl = (2/(k**2) + 2*fnl/(k**4))*Pdm[k]\n",
    "        Cfnl[k] = [[constant_fnl*(b1**2), constant_fnl*(r*b1*b2)], [constant_fnl*(r*b1*b2), constant_fnl*(b2**2)]]\n",
    "    derivatives_k = [Cb1, Cb2, Cfnl]\n",
    "    return derivatives_k #need to extract specific covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper method that adds together all the fisher matrices of different k values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(covariance, derivatives_k, k_array):\n",
    "    size = len(derivatives_k)\n",
    "    sum_fisher = np.zeros(shape=(size,size))\n",
    "    Cb1 = derivatives_k[0]\n",
    "    Cb2 = derivatives_k[1]\n",
    "    Cfnl = derivatives_k[2]\n",
    "    for k in k_array:\n",
    "        derivatives = [Cb1[k], Cb2[k], Cfnl[k]]\n",
    "        sum_fisher += fisher(derivatives, covariance[k])\n",
    "    return sum_fisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method uses the above methods to put together the fisher matrix after summing over all values of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffisher(b1, b2, r, fnl, k_array, Pdm, n1, n2):\n",
    "    covariance = covariance(b1, b2, r, fnl, k_array, Pdm, n1, n2)\n",
    "    derivatives_k = derivatives_1(b1, b2, r, fnl, k_array, Pdm)\n",
    "    fisher = helper(covariance, derivatives_k, k_array)\n",
    "    size = len(derivatives_k)\n",
    "    print(fisher.I[size - 1][size - 1]) #assuming fnl is the last index\n",
    "    return fisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to Uros's paper. He makes some assumptions to simplify his final expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
